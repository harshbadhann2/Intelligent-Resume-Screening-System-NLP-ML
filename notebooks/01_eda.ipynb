{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86a45370",
   "metadata": {},
   "source": [
    "# Resume Screening System - Exploratory Data Analysis\n",
    "\n",
    "This notebook explores resume and job description datasets, analyzing text characteristics and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b3e15",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from resume_screening.preprocessor import TextPreprocessor\n",
    "from resume_screening.data_loader import DataLoader, SyntheticDataGenerator, JobScraper\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c463948d",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Dataset for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045d3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic resume-job pairs for analysis\n",
    "resumes, jobs, labels = SyntheticDataGenerator.generate_matched_pairs(n_pairs=50)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'resume': resumes,\n",
    "    'job_description': jobs,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773aaac8",
   "metadata": {},
   "source": [
    "## 3. Analyze Text Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e23c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df['resume_length'] = df['resume'].str.len()\n",
    "df['job_length'] = df['job_description'].str.len()\n",
    "df['resume_words'] = df['resume'].str.split().str.len()\n",
    "df['job_words'] = df['job_description'].str.split().str.len()\n",
    "\n",
    "print(\"Resume Statistics:\")\n",
    "print(df['resume_length'].describe())\n",
    "print(f\"\\nJob Description Statistics:\")\n",
    "print(df['job_length'].describe())\n",
    "\n",
    "# Visualize text lengths\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(df['resume_words'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Number of Words')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Resume Length Distribution')\n",
    "\n",
    "axes[1].hist(df['job_words'], bins=30, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Words')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Job Description Length Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage resume words: {df['resume_words'].mean():.2f}\")\n",
    "print(f\"Average job description words: {df['job_words'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e0fbe",
   "metadata": {},
   "source": [
    "## 4. Preprocess and Tokenize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1ff711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(remove_stopwords=True, use_lemmatization=True)\n",
    "\n",
    "# Preprocess a sample\n",
    "sample_resume = df['resume'].iloc[0]\n",
    "sample_job = df['job_description'].iloc[0]\n",
    "\n",
    "print(\"Sample Resume (Original):\")\n",
    "print(sample_resume[:200] + \"...\")\n",
    "\n",
    "resume_tokens = preprocessor.process(sample_resume)\n",
    "print(f\"\\nTokens ({len(resume_tokens)}): {resume_tokens[:15]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nSample Job Description (Original):\")\n",
    "print(sample_job[:200] + \"...\")\n",
    "\n",
    "job_tokens = preprocessor.process(sample_job)\n",
    "print(f\"\\nTokens ({len(job_tokens)}): {job_tokens[:15]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead056e",
   "metadata": {},
   "source": [
    "## 5. Extract Skills and Key Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab7b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract skills from resumes\n",
    "df['skills_resume'] = df['resume'].apply(lambda x: TextPreprocessor.extract_skills(x))\n",
    "df['skills_job'] = df['job_description'].apply(lambda x: TextPreprocessor.extract_skills(x))\n",
    "\n",
    "print(\"Skills Extracted from Sample Resume:\")\n",
    "print(df['skills_resume'].iloc[0])\n",
    "\n",
    "print(\"\\nSkills Required in Sample Job:\")\n",
    "print(df['skills_job'].iloc[0])\n",
    "\n",
    "# Aggregate all skills\n",
    "all_resume_skills = []\n",
    "all_job_skills = []\n",
    "\n",
    "for skills_list in df['skills_resume']:\n",
    "    all_resume_skills.extend(skills_list)\n",
    "\n",
    "for skills_list in df['skills_job']:\n",
    "    all_job_skills.extend(skills_list)\n",
    "\n",
    "# Count frequencies\n",
    "resume_skill_counts = Counter(all_resume_skills)\n",
    "job_skill_counts = Counter(all_job_skills)\n",
    "\n",
    "print(\"\\nTop 10 Skills in Resumes:\")\n",
    "for skill, count in resume_skill_counts.most_common(10):\n",
    "    print(f\"  {skill}: {count}\")\n",
    "\n",
    "print(\"\\nTop 10 Skills in Job Descriptions:\")\n",
    "for skill, count in job_skill_counts.most_common(10):\n",
    "    print(f\"  {skill}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370095ac",
   "metadata": {},
   "source": [
    "## 6. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c94cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all documents\n",
    "resume_corpus = []\n",
    "job_corpus = []\n",
    "\n",
    "for resume in df['resume']:\n",
    "    tokens = preprocessor.process(resume)\n",
    "    resume_corpus.append(tokens)\n",
    "\n",
    "for job in df['job_description']:\n",
    "    tokens = preprocessor.process(job)\n",
    "    job_corpus.append(tokens)\n",
    "\n",
    "# Calculate vocabulary stats\n",
    "all_resume_tokens = [token for tokens in resume_corpus for token in tokens]\n",
    "all_job_tokens = [token for tokens in job_corpus for token in tokens]\n",
    "\n",
    "resume_vocab = set(all_resume_tokens)\n",
    "job_vocab = set(all_job_tokens)\n",
    "\n",
    "print(f\"Resume Corpus:\")\n",
    "print(f\"  Total tokens: {len(all_resume_tokens)}\")\n",
    "print(f\"  Unique tokens: {len(resume_vocab)}\")\n",
    "print(f\"  Vocabulary richness: {len(resume_vocab)/len(all_resume_tokens):.4f}\")\n",
    "\n",
    "print(f\"\\nJob Description Corpus:\")\n",
    "print(f\"  Total tokens: {len(all_job_tokens)}\")\n",
    "print(f\"  Unique tokens: {len(job_vocab)}\")\n",
    "print(f\"  Vocabulary richness: {len(job_vocab)/len(all_job_tokens):.4f}\")\n",
    "\n",
    "print(f\"\\nCommon tokens: {len(resume_vocab & job_vocab)}\")\n",
    "print(f\"Overlap: {len(resume_vocab & job_vocab) / len(resume_vocab | job_vocab):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8879551e",
   "metadata": {},
   "source": [
    "## 7. Most Frequent Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112bd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most common words\n",
    "resume_word_freq = Counter(all_resume_tokens)\n",
    "job_word_freq = Counter(all_job_tokens)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Top words in resumes\n",
    "top_resume_words = resume_word_freq.most_common(15)\n",
    "words, freqs = zip(*top_resume_words)\n",
    "axes[0].barh(words, freqs, color='skyblue')\n",
    "axes[0].set_xlabel('Frequency')\n",
    "axes[0].set_title('Top 15 Words in Resumes')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Top words in jobs\n",
    "top_job_words = job_word_freq.most_common(15)\n",
    "words, freqs = zip(*top_job_words)\n",
    "axes[1].barh(words, freqs, color='lightcoral')\n",
    "axes[1].set_xlabel('Frequency')\n",
    "axes[1].set_title('Top 15 Words in Job Descriptions')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8e6800",
   "metadata": {},
   "source": [
    "## 8. Label-based Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0bcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare matched vs non-matched pairs\n",
    "matched = df[df['label'] == 1]\n",
    "unmatched = df[df['label'] == 0]\n",
    "\n",
    "print(f\"Matched Pairs (label=1): {len(matched)}\")\n",
    "print(f\"  Average resume length: {matched['resume_words'].mean():.2f} words\")\n",
    "print(f\"  Average job length: {matched['job_words'].mean():.2f} words\")\n",
    "\n",
    "print(f\"\\nUnmatched Pairs (label=0): {len(unmatched)}\")\n",
    "print(f\"  Average resume length: {unmatched['resume_words'].mean():.2f} words\")\n",
    "print(f\"  Average job length: {unmatched['job_words'].mean():.2f} words\")\n",
    "\n",
    "# Skills overlap\n",
    "matched['skill_overlap'] = matched.apply(\n",
    "    lambda row: len(set(row['skills_resume']) & set(row['skills_job'])), \n",
    "    axis=1\n",
    ")\n",
    "unmatched['skill_overlap'] = unmatched.apply(\n",
    "    lambda row: len(set(row['skills_resume']) & set(row['skills_job'])), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nMatched - Average skill overlap: {matched['skill_overlap'].mean():.2f}\")\n",
    "print(f\"Unmatched - Average skill overlap: {unmatched['skill_overlap'].mean():.2f}\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "data_to_plot = [matched['skill_overlap'], unmatched['skill_overlap']]\n",
    "ax.boxplot(data_to_plot, labels=['Matched', 'Unmatched'])\n",
    "ax.set_ylabel('Skill Overlap Count')\n",
    "ax.set_title('Skill Overlap: Matched vs Unmatched Pairs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fcbe80",
   "metadata": {},
   "source": [
    "## 9. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b340ad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "print(f\"\\nDuplicate resumes: {df['resume'].duplicated().sum()}\")\n",
    "print(f\"Duplicate job descriptions: {df['job_description'].duplicated().sum()}\")\n",
    "\n",
    "# Check text quality\n",
    "print(f\"\\nTexts with very short length (<10 words):\")\n",
    "print(f\"  Resumes: {(df['resume_words'] < 10).sum()}\")\n",
    "print(f\"  Jobs: {(df['job_words'] < 10).sum()}\")\n",
    "\n",
    "print(f\"\\nTexts with very long length (>500 words):\")\n",
    "print(f\"  Resumes: {(df['resume_words'] > 500).sum()}\")\n",
    "print(f\"  Jobs: {(df['job_words'] > 500).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2974cb54",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc365ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "EXPLORATORY DATA ANALYSIS SUMMARY\n",
    "==================================\n",
    "\n",
    "Dataset Overview:\n",
    "- Total samples: {} pairs\n",
    "- Matched: {} ({:.1f}%)\n",
    "- Unmatched: {} ({:.1f}%)\n",
    "\n",
    "Text Characteristics:\n",
    "- Average resume length: {:.0f} words\n",
    "- Average job length: {:.0f} words\n",
    "- Common vocabulary: {:.1f}% overlap\n",
    "\n",
    "Key Findings:\n",
    "- Skills are strong indicators of resume-job matching\n",
    "- Longer texts tend to have more diverse vocabulary\n",
    "- Technical terms appear frequently in both corpus\n",
    "\n",
    "Next Steps:\n",
    "1. Move to Notebook 02: Preprocessing & Feature Engineering\n",
    "2. Implement TF-IDF, Word2Vec, BERT embeddings\n",
    "3. Create similarity scoring baseline\n",
    "4. Train classification models for ranking\n",
    "\"\"\".format(\n",
    "    len(df),\n",
    "    (df['label']==1).sum(),\n",
    "    (df['label']==1).sum()/len(df)*100,\n",
    "    (df['label']==0).sum(),\n",
    "    (df['label']==0).sum()/len(df)*100,\n",
    "    df['resume_words'].mean(),\n",
    "    df['job_words'].mean(),\n",
    "    len(resume_vocab & job_vocab) / len(resume_vocab | job_vocab) * 100\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
