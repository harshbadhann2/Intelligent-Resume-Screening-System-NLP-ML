{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ad270c",
   "metadata": {},
   "source": [
    "# Resume Screening - Embeddings & Feature Generation\n",
    "\n",
    "Generate embeddings using TF-IDF, Word2Vec, and BERT, then combine them for feature representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d1a741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from resume_screening.embeddings import TFIDFEmbedder, Word2VecEmbedder, BERTEmbedder\n",
    "from resume_screening.data_loader import SyntheticDataGenerator\n",
    "from resume_screening.utils import PerformanceMonitor\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565446b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "resumes, jobs, labels = SyntheticDataGenerator.generate_matched_pairs(n_pairs=100)\n",
    "\n",
    "print(f\"Dataset created: {len(resumes)} samples\")\n",
    "print(f\"Sample resume: {resumes[0][:100]}...\")\n",
    "print(f\"Sample job: {jobs[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3911fb",
   "metadata": {},
   "source": [
    "## 1. TF-IDF Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd96414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TF-IDF\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "monitor.start('tfidf_train')\n",
    "tfidf = TFIDFEmbedder(max_features=1000)\n",
    "tfidf.train(resumes + jobs)  # Train on both resumes and jobs\n",
    "monitor.end('tfidf_train')\n",
    "\n",
    "# Get embeddings\n",
    "monitor.start('tfidf_embed')\n",
    "resume_emb_tfidf = tfidf.embed(resumes[:10])\n",
    "monitor.end('tfidf_embed')\n",
    "\n",
    "print(f\"TF-IDF Training Time: {monitor.times['tfidf_train']['elapsed']:.4f}s\")\n",
    "print(f\"TF-IDF Embedding Time (10 samples): {monitor.times['tfidf_embed']['elapsed']:.4f}s\")\n",
    "print(f\"TF-IDF Embedding Shape: {resume_emb_tfidf.shape}\")\n",
    "print(f\"Sparsity: {(resume_emb_tfidf == 0).sum() / resume_emb_tfidf.size:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb6dd1",
   "metadata": {},
   "source": [
    "## 2. Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efc7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec\n",
    "monitor.start('word2vec_train')\n",
    "w2v = Word2VecEmbedder(vector_size=300, window=5)\n",
    "w2v.train(resumes + jobs, epochs=10)\n",
    "monitor.end('word2vec_train')\n",
    "\n",
    "# Get embeddings\n",
    "monitor.start('word2vec_embed')\n",
    "resume_emb_w2v = w2v.embed(resumes[:10])\n",
    "monitor.end('word2vec_embed')\n",
    "\n",
    "print(f\"Word2Vec Training Time: {monitor.times['word2vec_train']['elapsed']:.4f}s\")\n",
    "print(f\"Word2Vec Embedding Time (10 samples): {monitor.times['word2vec_embed']['elapsed']:.4f}s\")\n",
    "print(f\"Word2Vec Embedding Shape: {resume_emb_w2v.shape}\")\n",
    "print(f\"Word2Vec Model Vocabulary: {len(w2v.model.wv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13eca13b",
   "metadata": {},
   "source": [
    "## 3. BERT Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6c9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT\n",
    "monitor.start('bert_load')\n",
    "bert = BERTEmbedder(model_name='all-MiniLM-L6-v2')\n",
    "monitor.end('bert_load')\n",
    "\n",
    "# Get embeddings\n",
    "monitor.start('bert_embed')\n",
    "resume_emb_bert = bert.embed(resumes[:10])\n",
    "monitor.end('bert_embed')\n",
    "\n",
    "print(f\"BERT Load Time: {monitor.times['bert_load']['elapsed']:.4f}s\")\n",
    "print(f\"BERT Embedding Time (10 samples): {monitor.times['bert_embed']['elapsed']:.4f}s\")\n",
    "print(f\"BERT Embedding Shape: {resume_emb_bert.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4e7518",
   "metadata": {},
   "source": [
    "## 4. Embedding Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7382ce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Embedding statistics\n",
    "embeddings = {\n",
    "    'TF-IDF': resume_emb_tfidf,\n",
    "    'Word2Vec': resume_emb_w2v,\n",
    "    'BERT': resume_emb_bert\n",
    "}\n",
    "\n",
    "# Shape comparison\n",
    "shapes = {k: v.shape for k, v in embeddings.items()}\n",
    "axes[0, 0].bar(shapes.keys(), [s[1] for s in shapes.values()], color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[0, 0].set_ylabel('Embedding Dimension')\n",
    "axes[0, 0].set_title('Embedding Dimensions')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Norm distribution\n",
    "for emb_name, emb in embeddings.items():\n",
    "    norms = np.linalg.norm(emb, axis=1)\n",
    "    axes[0, 1].hist(norms, alpha=0.5, label=emb_name, bins=20)\n",
    "axes[0, 1].set_xlabel('L2 Norm')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Embedding Norm Distribution')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# PCA visualization (2D)\n",
    "for emb_name, emb in embeddings.items():\n",
    "    pca = PCA(n_components=2)\n",
    "    emb_2d = pca.fit_transform(emb)\n",
    "    axes[1, 0].scatter(emb_2d[:, 0], emb_2d[:, 1], alpha=0.6, label=emb_name, s=50)\n",
    "axes[1, 0].set_xlabel('PC1')\n",
    "axes[1, 0].set_ylabel('PC2')\n",
    "axes[1, 0].set_title('PCA 2D Projection')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Sparsity\n",
    "sparsity_data = []\n",
    "for emb_name, emb in embeddings.items():\n",
    "    sparsity = (emb == 0).sum() / emb.size\n",
    "    sparsity_data.append(sparsity)\n",
    "axes[1, 1].bar(embeddings.keys(), sparsity_data, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "axes[1, 1].set_ylabel('Sparsity')\n",
    "axes[1, 1].set_title('Embedding Sparsity')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Embedding Statistics:\")\n",
    "for emb_name, emb in embeddings.items():\n",
    "    print(f\"\\n{emb_name}:\")\n",
    "    print(f\"  Shape: {emb.shape}\")\n",
    "    print(f\"  Mean norm: {np.linalg.norm(emb, axis=1).mean():.4f}\")\n",
    "    print(f\"  Sparsity: {(emb == 0).sum() / emb.size:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164040b2",
   "metadata": {},
   "source": [
    "## 5. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2c6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "tfidf.save('../models/tfidf_model')\n",
    "w2v.save('../models/word2vec_model')\n",
    "bert.save('../models/bert_model')\n",
    "\n",
    "print(\"All models saved!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
